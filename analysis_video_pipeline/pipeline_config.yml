nodes:

# Use saved video feed from raw_video_pipeline
- input.visual:
    source: 0

# Add YOLOv4 model for human detection to obtain bounding box information
- model.yolo:
    score_threshold: 0.6
# Use the bounding box info to track and count detected humans
- dabble.tracking
- dabble.statistics:
    maximum: obj_attrs["ids"]
# Draw the resultant bounding boxes and tracking IDs
# - draw.bbox
- draw.tag:
    show: ["ids"]

# Add PoseNet model for human pose estimation
- model.posenet:
    score_threshold: 0.6
# Draw the resultant poses
- draw.poses

# Add custom Node model for pose analysis
- custom_nodes.dabble.movement

# Display metadata
- dabble.fps
- draw.legend:
    show: ["cum_max", "fps"]

# Generate output video feed
- output.screen:
    window_name: "SpeakEase"
- output.media_writer:
    output_dir: analysed_videos